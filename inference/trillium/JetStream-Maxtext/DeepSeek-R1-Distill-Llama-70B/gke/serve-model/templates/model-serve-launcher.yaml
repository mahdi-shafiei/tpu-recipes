# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

{{- $root := . }}

apiVersion: apps/v1
kind: Deployment

metadata:
  name: {{ .Release.Name }}-serving
  labels:
    app: {{ .Release.Name }}-serving
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ .Release.Name }}-serving
  template:
    metadata:
      labels:
        app: {{ .Release.Name }}-serving
      annotations:
        kubectl.kubernetes.io/default-container: serving
        {{- if .Values.volumes.gcsMounts }}
        gke-gcsfuse/volumes: "true"
        gke-gcsfuse/cpu-limit: "0"
        gke-gcsfuse/memory-limit: "0"
        gke-gcsfuse/ephemeral-storage-limit: "0"
        {{- end }}
    spec:
      restartPolicy: Always
      nodeSelector:
        cloud.google.com/gke-tpu-topology: 2x4
        cloud.google.com/gke-tpu-accelerator: tpu-v6e-slice
      tolerations:
      - key: google.com/tpu
        operator: Exists
      - key: cloud.google.com/impending-node-termination
        operator: Exists
      volumes:
        - name: shared-memory
          emptyDir:
            medium: "Memory"
            sizeLimit: 250Gi
        - name: local-ssd
          hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd
        - name: workload-configuration
          configMap:
            name: "{{.Release.Name}}"
        {{- range $gcs := .Values.volumes.gcsMounts }}
        - name: "{{ $gcs.bucketName }}"
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: "{{ $gcs.bucketName }}"
              mountOptions: "implicit-dirs,file-cache:enable-parallel-downloads:true,file-cache:parallel-downloads-per-file:100,file-cache:max-parallel-downloads:-1,file-cache:download-chunk-size-mb:10,file-cache:max-size-mb:-1"
        {{- end }}

      containers:
      - name: model-serving
        image: "{{ .Values.job.image.repository }}:{{ .Values.job.image.tag }}"
        imagePullPolicy: Always
        securityContext:
          privileged: true
        resources:
          requests:
            google.com/tpu: 8
          limits:
            google.com/tpu: 8
        ports:
          - containerPort: {{ .Values.jetstream.service.ports.grpc }}
        env:
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: "{{ .Values.huggingface.secretName }}"
                key: "{{ .Values.huggingface.secretData.token }}"
          # Enable faster downloads of model weights from HuggingFace
          # - name: HF_HUB_ENABLE_HF_TRANSFER
          #   value: "1"
          - name: BASE_MODEL_PATH
            value: "/gcs/{{ .Values.model.name }}"
          - name: TOKENIZER_DIR
            value: "/maxtext/assets/tokenizer_llama3.tiktoken"
          - name: CHECKPOINT_TPU_SCANNED
            value: "/gcs/{{ .Values.model.name }}/scanned_ckpt"
          - name: RUN_NAME
            value: "unscanned_ckpt"
          - name: CHECKPOINT_TPU_UNSCANNED
            value: "/gcs/{{ .Values.model.name }}/output/unscanned_ckpt/checkpoints/0/items"
          - name: LOAD_PARAMETERS_PATH
            value: "/gcs/{{ .Values.model.name }}/scanned_ckpt/0/items"
          - name: MODEL_SIZE
            value: "llama3.1-70b"
          {{- range $gcs := $root.Values.volumes.gcsMounts }}
          - name: GCS_FUSE_BUCKET
            value: "{{ $gcs.bucketName }}"
          {{- end }}

          - name: LIBTPU_INIT_ARGS
            valueFrom:
              configMapKeyRef:
                name: "{{ .Release.Name }}"
                key: libtpu-init-args

        workingDir: /workspace
        command: ["/bin/bash", "-c"]
        args:
          - |
            set -eux

            pip install torch --index-url https://download.pytorch.org/whl/cpu
            python3 -m nltk.downloader punkt_tab

            # echo $LIBTPU_INIT_ARGS

            # # Download MMLU dataset
            # apt update && apt install -y wget
            # mkdir -p /gcs/benchmarks
            # mkdir -p /gcs/mmlu
            # cd /gcs/mmlu
            # wget https://people.eecs.berkeley.edu/~hendrycks/data.tar -P /gcs/mmlu
            # tar -xvf data.tar

            # Download HuggingFace model
            # huggingface-cli download {{ .Values.model.name }} --local-dir ${BASE_MODEL_PATH}

            # Parse server configurations from values file
            echo "MaxText configuration file:"
            sed 's/^/| /' /etc/workload-configuration/maxtext-configuration.yaml
            echo ""

            OPTIONS=()
            while IFS= read -r line || [[ -n "$line" ]]; do
              # Skip empty lines and comments
              [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue
              
              key=$(echo "$line" | cut -d':' -f1 | tr -d '[:space:]')
              value=$(echo "$line" | cut -d':' -f2- | sed 's/^[[:space:]]*//')
              
              # Handle environment variable expansion
              if [[ "$value" == \$* ]]; then
                var_name=${value#\$}
                
                if [[ -z "$var_name" ]]; then
                  expanded_value="$"
                else
                  expanded_value="${!var_name:-$value}"
                fi
                
                OPTIONS+=("$key=$expanded_value")
              else
                OPTIONS+=("$key=$value")
              fi
            done < /etc/workload-configuration/maxtext-configuration.yaml

            echo "===== MaxText Configuration ====="
            echo "${OPTIONS[@]}"

            # Step 1: Convert HuggingFace model to MaxText model
            {{- if .Values.convert_hf_ckpt }}
            # Convert HuggingFace model to MaxText model
            JAX_PLATFORMS=cpu python3 /maxtext/MaxText/llama_or_mistral_ckpt.py \
              --base-model-path ${BASE_MODEL_PATH} \
              --maxtext-model-path ${CHECKPOINT_TPU_SCANNED} \
              --model-size ${MODEL_SIZE} \
              --huggingface-checkpoint True

            # Step 2: Convert scanned checkpoint to unscanned checkpoint for better serving performance
            JAX_PLATFORMS=cpu python3 /maxtext/MaxText/generate_param_only_checkpoint.py \
              /maxtext/MaxText/configs/base.yml \
              async_checkpointing=false \
              base_output_directory=${BASE_MODEL_PATH}/output \
              load_parameters_path=${CHECKPOINT_TPU_SCANNED}/0/items \
              run_name=${RUN_NAME} \
              model_name=${MODEL_SIZE} \
              force_unroll=true
            {{- end }}


            # Step 3: Start the JetStream MaxText server
            python3 /maxtext/MaxText/maxengine_server.py \
              /maxtext/MaxText/configs/base.yml \
              "${OPTIONS[@]}"

        volumeMounts:
          - name: shared-memory
            mountPath: /dev/shm
          - name: workload-configuration
            mountPath: /etc/workload-configuration
          - name: local-ssd
            mountPath: {{ .Values.volumes.ssdMountPath }}
          {{- range $gcs := .Values.volumes.gcsMounts }}
          - name: "{{ $gcs.bucketName }}"
            mountPath: "{{ $gcs.mountPath }}"
          {{- end }}